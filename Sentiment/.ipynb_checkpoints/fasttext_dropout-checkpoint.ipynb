{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import sequence\n",
    "from keras import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, LSTM, CuDNNLSTM, Dense, Dropout, TimeDistributed, Activation, Conv1D, MaxPooling1D\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vector_possible = np.load('no_vector_possible.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "# remove rows without label\n",
    "df = df[df.label != 0]\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract text and according labels\n",
    "text = df['text_lower'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Show number of training examples\n",
    "print(\"Number of texts:\",len(labels),\"\\n\")\n",
    "\n",
    "# Tokenize - Vocab to Int mapping dictionary\n",
    "all_text = ' '.join(text)\n",
    "# Create a list of words\n",
    "words = all_text.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "# Create the actuall mapping\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "print(\"Number of words:\",len(vocab_to_int))\n",
    "\n",
    "\n",
    "# Remove Stopwords\n",
    "set_words = set(words)\n",
    "stopwords_int = [vocab_to_int[w] for w in stopwords if w in set_words]\n",
    "\n",
    "\n",
    "text_int = []\n",
    "for paragraph in text:\n",
    "    words_that_have_vec = [word for word in paragraph.split() if word not in no_vector_possible]\n",
    "    p = [vocab_to_int[w]-1 for w in words_that_have_vec]# if w not in stopwords_int]\n",
    "    text_int.append(p)\n",
    "    \n",
    "    \n",
    "# Encode labels from -1(Negativ), 0(Positiv), 1(Positiv)\n",
    "labels = [int(x)-1 for x in labels]    \n",
    "\n",
    "# One Hot encoding\n",
    "encoded_labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "# Pad the features into constant length lists\n",
    "max_words = 200\n",
    "features = sequence.pad_sequences(text_int, maxlen = max_words)\n",
    "\n",
    "# Split the set into training (8/10) and testing data (2/10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                features, encoded_labels, test_size=0.2, shuffle= True)\n",
    "\n",
    "\n",
    "# Check how many traing/ testing samples there are\n",
    "print(\"\\n\\nTraining Examples:\",len(X_train),\"\\nTesting Examples:\",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrix\n",
    "embed_dim = 300 \n",
    "MAX_NB_WORDS = 100000\n",
    "words_not_found = []\n",
    "\n",
    "#Crteating the EmbeddingMatrixFrame\n",
    "print('preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(vocab_to_int))\n",
    "embedding_matrix = np.load('embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import L1L2\n",
    "from keras import backend as K\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "# Model architecture\n",
    "def create_model(dropout):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(40000, 100, input_length=200))\n",
    "    model.add(CuDNNLSTM(200))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# save the hist created by the different models\n",
    "def save_hist(hist,key):\n",
    "    acc = hist.history['acc']\n",
    "    vall_acc = hist.history['val_acc']\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    hist= [acc,vall_acc,loss,val_loss]\n",
    "    np.save(\"{}_init_model\".format(key),np.array(hist))\n",
    "\n",
    "\n",
    "hist_list = []\n",
    "# for each of the dropout rates\n",
    "for dropout in [0.0, 0.2, 0.5, 0.7]:\n",
    "    # Create model\n",
    "    model = create_model(dropout)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit model\n",
    "    hist = model.fit(X_train, y_train,\n",
    "          validation_split=0.10,\n",
    "          batch_size=16,\n",
    "          epochs=30)\n",
    "    \n",
    "    # append the history to the list \n",
    "    hist_list.append(hist)\n",
    "    save_hist(hist,dropout)\n",
    "    \n",
    "    # Evaluation of the model\n",
    "    print(\"Calculating Accuracy...\")\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose = 1)\n",
    "    print(\"\\nAccuracy: {} - ({})\".format(round(acc,2), acc))\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the accuracys and model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dropout graphs\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(13,5))\n",
    "ax0 = ax[0]\n",
    "ax1 = ax[1]\n",
    "\n",
    "for hist_obj in hist_list:\n",
    "    \n",
    "    z = np.polyfit(range(len(hist_obj.history['val_acc'])), hist_obj.history['val_acc'], 3)\n",
    "    f = np.poly1d(z)\n",
    "    \n",
    "    x_new = np.linspace(0, 30)\n",
    "    y_new = f(x_new)\n",
    "    \n",
    "    \n",
    "    #ax0.plot(hist_obj.history['acc'])\n",
    "    #ax0.plot(hist_obj.history['val_acc'])\n",
    "    ax0.plot(x_new,y_new)\n",
    "    \n",
    "    \n",
    "    z = np.polyfit(range(len(hist_obj.history['val_loss'])), hist_obj.history['val_loss'], 3)\n",
    "    f = np.poly1d(z)\n",
    "    \n",
    "    x_new = np.linspace(0, 30)\n",
    "    y_new = f(x_new)\n",
    "    \n",
    "    \n",
    "    # summarize history for loss\n",
    "    #ax1.plot(hist_obj.history['loss'])\n",
    "    #ax1.plot(hist_obj.history['val_loss'])\n",
    "    ax1.plot(x_new,y_new)\n",
    "\n",
    "    \n",
    "ax0.set_title('model accuracy')\n",
    "ax0.set_ylabel('accuracy')\n",
    "ax0.set_xlabel('epoch')\n",
    "ax0.legend([\"0.0\", \"0.2\", \"0.5\", \"0.7\"], title=\"Dropout rate\")\n",
    "\n",
    "    \n",
    "ax1.set_title('model loss')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend([\"0.0\", \"0.2\", \"0.5\", \"0.7\"], title=\"Dropout rate\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
