{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataframe\n",
    "Create the dataset given the five files \n",
    "\n",
    "\"data_aerzteblatt.csv\"\n",
    "\n",
    "\"data_apotheke-adhoc.csv\"\n",
    "\n",
    "\"data_apotheken-umschau.csv\"\n",
    "\n",
    "\"data_deutsche-apotheker-zeitung.csv\"\n",
    "\n",
    "\"data_pharmazeutische-zeitung.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Read in the documents\n",
    "\n",
    "# List with all the csv files, containing article data\n",
    "document_names = [\"data_aerzteblatt.csv\", \n",
    "                  \"data_apotheke-adhoc.csv\", \n",
    "                  \"data_apotheken-umschau.csv\", \n",
    "                  \"data_deutsche-apotheker-zeitung.csv\", \n",
    "                  \"data_pharmazeutische-zeitung.csv\"]\n",
    "\n",
    "# print out all the documents that ate going to be loaded\n",
    "for i, document_name in enumerate(document_names,0):\n",
    "    print(\"Document_name_{}: {}\".format(i,document_name))\n",
    "    \n",
    "# create dataframes for all the documents\n",
    "df1 = pd.read_csv(document_names[0])\n",
    "df2 = pd.read_csv(document_names[1])\n",
    "df3 = pd.read_csv(document_names[2])\n",
    "df4 = pd.read_csv(document_names[3])\n",
    "df5 = pd.read_csv(document_names[4])\n",
    "\n",
    "# make a list of all the dataframe objects\n",
    "frames = [df1, df2, df3, df4, df5]\n",
    "\n",
    "# Create one big dataframe by conncatenating all dataframes in frames\n",
    "document = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "# remove the quotationmarks from the product name\n",
    "def remove_quotations(product):\n",
    "    return product[1:-1]\n",
    "\n",
    "\n",
    "# remove nan\n",
    "document = document.dropna()\n",
    "# remove articles that have no modification value\n",
    "document = document.drop('isModified', axis=1)\n",
    "# remove articles that are written in another language than german or not specified\n",
    "document = document[document[\"language\"] == \"de\"]\n",
    "document = document.drop('language', axis=1)\n",
    "# remove articles without a description\n",
    "document = document.drop('description', axis=1)\n",
    "# remove documents that don't specify the maket launch month\n",
    "document = document[document[\"month\"] != 0]\n",
    "# remove all artilces without a text\n",
    "document = document.dropna(subset=[\"text\"])\n",
    "\n",
    "document = document[~document.url.str.contains(\"Beipackzettel\")]\n",
    "document = document[~document.url.str.contains(\"unternehmenskommunikation\")]\n",
    "\n",
    "\n",
    "#  remove the quotationmarks from the product name\n",
    "document[\"product\"] = document[\"product\"].apply(remove_quotations)\n",
    "# reset the index of the dataframe\n",
    "document = document.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 2: Convert text to lower case \n",
    "document[\"text_lower\"] = document['text'].apply(str.lower)\n",
    "\n",
    "# STEP 3: Remove punctuation and other symbols\n",
    "\n",
    "# Create function to remove punctuation and other symbols\n",
    "def trans(text):\n",
    "    \"\"\"\n",
    "    Function that removes specific chartacters from a text\n",
    "    \n",
    "    text: the text the characters are removed from\n",
    "    \"\"\"\n",
    "    for symbol in [string.punctuation,\"–\",\"„\",\"“\",\"©\",\"(\",\")\",\"-\"]:\n",
    "        text = text.translate(str.maketrans('', '', symbol))\n",
    "    return text\n",
    "\n",
    "\n",
    "# Remove the punctuation and other symbols\n",
    "document[\"text_lower\"] = document[\"text_lower\"].apply(trans)\n",
    "\n",
    "# create a index for the dataframe\n",
    "document[\"index\"] = range(0, len(document))\n",
    "\n",
    "# get the column name sof the dataframe\n",
    "cols = document.columns.tolist()\n",
    "# rearange the columns\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "document = document[cols]\n",
    "\n",
    "# create document that has all the articles\n",
    "document.to_csv(\"document.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset for the paragraphs\n",
    "dataset = pd.DataFrame(columns = [\"substance\",\"product\", \"year\", \"month\", \"title\", \"date_published\", \"url\", \"text_index\",\"text\", \"text_lower\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs(text, text_lower):\n",
    "    \"\"\"\n",
    "    Generator object takes the text and the text_lower to split it into its paragraphs and yield them one after another\n",
    "    \"\"\"\n",
    "    # use the \\n symbol to slit the text into its paragraphs\n",
    "    paras = text.split(\"\\n\")\n",
    "    paras_lower = text_lower.split(\"\\n\")\n",
    "    \n",
    "    # zip the paragraphs from text and text_lower together to yield them\n",
    "    for para,para_lower in zip(paras,paras_lower):\n",
    "        yield para, para_lower\n",
    "    \n",
    "# Create an Iterator object from the document\n",
    "iterator = document.iterrows()\n",
    "\n",
    "i=0\n",
    "# for each row in the document\n",
    "for row in iterator:\n",
    "    # loop over the paragraphs of the article\n",
    "    for paragraph, paragraph_lower in paragraphs(row[1][\"text\"],row[1][\"text_lower\"]):\n",
    "        # if the partagraph is bigger than 5 words, add it to the dataframe\n",
    "        if len(paragraph.split()) >= 5:\n",
    "            dataset.loc[i] = [row[1][\"substance\"],row[1][\"product\"],row[1][\"year\"],row[1][\"month\"],row[1][\"title\"],row[1][\"date_published\"],row[1][\"url\"], row[1][\"index\"],paragraph,paragraph_lower, 0]\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorparate the ebaid number from the file ebaid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ebaid file\n",
    "ebaid = pd.read_csv(\"ebaid.csv\", sep=';')\n",
    "\n",
    "# create a dictionary object\n",
    "ebaid_dict = {}\n",
    "# fill the ebaid_dict with entries, the key is the substance name and the vlaue is a list with the \"ebaid\", \"publication date \" and \"decision date\"\n",
    "for substance, eid in zip(ebaid['substance'].tolist(),zip(ebaid['ebaid'].tolist(),ebaid['dpublication'].tolist(),ebaid['ddecision'].tolist())):\n",
    "    ebaid_dict[substance] = eid\n",
    "\n",
    "# create a dictionary object\n",
    "ebaid_dict_2 = {} \n",
    "# fill the ebaid_dict with entries, the key is the product name and the vlaue is a list with the \"ebaid\", \"publication date \" and \"decision date\"\n",
    "for product, eid in zip(ebaid['brandname'].tolist(),zip(ebaid['ebaid'].tolist(),ebaid['dpublication'].tolist(),ebaid['ddecision'].tolist())):\n",
    "    ebaid_dict_2[product] = eid\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Create three list objects for later\n",
    "ebaid_list_0=[]\n",
    "ebaid_list_1=[]\n",
    "ebaid_list_2=[]\n",
    "\n",
    "#create a new dataframe object\n",
    "dataset_short = pd.DataFrame()\n",
    "\n",
    "# for each substance and productname in the dataset\n",
    "for i, (substance,product) in enumerate(zip(np.unique(dataset['substance'].tolist()),np.unique(dataset['product'].tolist()))):\n",
    "    # check if the substance is part of the ebaid_dict\n",
    "    try:\n",
    "        ebaid_list = ebaid_dict[substance]\n",
    "    except:\n",
    "        # if it is not, check if the product anme is part of the ebaid_dict_2\n",
    "        try:\n",
    "            ebaid_list = ebaid_dict_2[product]\n",
    "        except:\n",
    "            # if it is not, use blank spaces and higher the counter ( to know how many substances could not be found in the ebaid.csv)\n",
    "            ebaid_list = [\"\",\"\",\"\"]\n",
    "            counter+=1\n",
    "    # add the values of the dictionary key to the three lists created earlier  \n",
    "    ebaid_list_0.append(ebaid_list[0])\n",
    "    ebaid_list_1.append(ebaid_list[1])\n",
    "    ebaid_list_2.append(ebaid_list[2])\n",
    "    \n",
    "# cretae a dataset that has all different substance names in it and add the ebaid, publication date and decision date to it\n",
    "dataset_short['substance'] = np.unique(dataset['substance'].tolist())\n",
    "dataset_short['ebaid'] =   ebaid_list_0\n",
    "dataset_short['dpublication'] =   ebaid_list_1\n",
    "dataset_short['ddecision'] =   ebaid_list_2\n",
    "\n",
    "# Show how many substance ebnaids have not been found\n",
    "print(\"Leftover:\",counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a transformed version of the dataset_short and make a dictionary out of it iwth the substance name as key and the rest as value\n",
    "ds_T = dataset_short.set_index('substance').T.to_dict('list')\n",
    "\n",
    "# make a list from the substance column in the dataset\n",
    "l =[]\n",
    "# Append the list with the values of ds_T for the specific key\n",
    "for i, substance in enumerate(dataset['substance'].tolist()):\n",
    "    l.append(ds_T[substance])\n",
    "\n",
    "# add the values to the dataset\n",
    "dataset[\"ebaid\"] = [item[0] for item in l]\n",
    "dataset[\"dpublication\"] = [item[1] for item in l]\n",
    "dataset[\"ddecision\"] = [item[2] for item in l]\n",
    "dataset.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leftover\n",
    "The 10 substance that are not found in ebaid.csv are looked up manually and added to the dataset.csv.\n",
    "\n",
    "Furthermore the dataset was manually searched for text that are no actuall news articles and thoise rows were deleted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
